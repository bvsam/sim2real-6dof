"""
PyTorch Dataset for NOCS-based 6DoF pose estimation.
Reads HDF5 files generated by the synthetic data pipeline.
"""

import json
from pathlib import Path
from typing import Dict, List, Optional

import h5py
import numpy as np
import torch
from torch.utils.data import Dataset


class NOCSDataset(Dataset):
    """
    Dataset for NOCS prediction from synthetic HDF5 files.

    Follows the format of the original NOCS paper's dataset.
    Each sample contains one object instance (single-instance per image).
    """

    def __init__(
        self,
        data_dir: str,
        split: str = "train",
        include_negatives: bool = False,
        load_poses: bool = False,
        transform: Optional[callable] = None,
        flip_z_axis: bool = True,  # Match original NOCS convention
    ):
        """
        Args:
            data_dir: Directory containing HDF5 files (e.g., 'output/')
            split: Dataset split ('train', 'val', 'test')
            include_negatives: Whether to include negative samples (background only)
            load_poses: Whether to load ground truth poses (R, t) for validation
            transform: Optional transforms to apply
        """
        self.data_dir = Path(data_dir)
        self.split = split
        self.include_negatives = include_negatives
        self.load_poses = load_poses
        self.transform = transform
        self.flip_z_axis = flip_z_axis

        # Find all HDF5 files
        self.hdf5_files = sorted(self.data_dir.glob("*.hdf5"))

        if not self.hdf5_files:
            raise RuntimeError(f"No HDF5 files found in {self.data_dir}")

        # Build index
        self.samples = self._build_index()

        print(
            f"[{split}] Loaded {len(self.samples)} samples from {len(self.hdf5_files)} HDF5 files"
        )

    def _build_index(self) -> List[Path]:
        """Build index of all valid samples."""
        samples = []

        for hdf5_path in self.hdf5_files:
            with h5py.File(hdf5_path, "r") as f:
                is_negative = f.get("is_negative", [np.array([0])])[()]
                if isinstance(is_negative, np.ndarray):
                    is_negative = is_negative[0] if is_negative.size > 0 else 0

                if is_negative and not self.include_negatives:
                    continue

                samples.append(hdf5_path)

        return samples

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict:
        """
        Load a single sample.

        Returns dict with format matching original NOCS dataset:
            - image: RGB (H, W, 3), uint8
            - masks: Instance masks (H, W, 1), bool - note: single instance
            - coords: NOCS maps (H, W, 1, 3), float32 [0, 1]
            - class_ids: Class labels (1,), int64 - note: always 0 (mug)
            - scales: Object scale (1, 3), float32
            - domain_label: 0 (has NOCS) or 1 (no NOCS)
            - intrinsics: Camera K (3, 3), float32

        Optional (if load_poses=True):
            - rotation: (3, 3), float32
            - translation: (3,), float32
        """
        hdf5_path = self.samples[idx]

        with h5py.File(hdf5_path, "r") as f:
            # RGB image (H, W, 3 or 4) -> (H, W, 3)
            rgb = np.array(f["colors"])
            if rgb.shape[-1] == 4:
                rgb = rgb[:, :, :3]

            # Check if negative sample
            is_negative = f.get("is_negative", [np.array([0])])[()]
            if isinstance(is_negative, np.ndarray):
                is_negative = is_negative[0] if is_negative.size > 0 else 0

            if is_negative:
                # Return empty masks/coords for negative samples
                h, w = rgb.shape[:2]
                masks = np.zeros((h, w, 0), dtype=bool)
                coords = np.zeros((h, w, 0, 3), dtype=np.float32)
                class_ids = np.array([], dtype=np.int64)
                scales = np.zeros((0, 3), dtype=np.float32)
                domain_label = 1  # No NOCS for negatives
            else:
                # Instance mask (H, W) -> (H, W, 1)
                mask = np.array(f["instance_mask"])
                masks = (mask > 0)[:, :, np.newaxis]

                # NOCS map (H, W, 3) -> (H, W, 1, 3)
                nocs = np.array(f["nocs"])[:, :, :3]

                if self.flip_z_axis:
                    # CRITICAL: Flip Z-axis to match official NOCS format
                    nocs[:, :, 2] = 1 - nocs[:, :, 2]

                # Mask out background and add instance dimension
                coords = (nocs * masks).astype(np.float32)[:, :, np.newaxis, :]

                # Class ID (always 0 for mug in single-class case)
                class_ids = np.array([0], dtype=np.int64)

                # Scale from metadata
                metadata_raw = f["metadata"][()]
                if isinstance(metadata_raw, bytes):
                    metadata_str = metadata_raw.decode("utf-8")
                elif isinstance(metadata_raw, np.bytes_):
                    metadata_str = metadata_raw.decode("utf-8")
                else:
                    metadata_str = str(metadata_raw)
                metadata = json.loads(metadata_str)
                object_scale = metadata.get("object_scale", 1.0)

                # Scales shape: (1, 3) - uniform scale in all dimensions
                scales = np.array(
                    [[object_scale, object_scale, object_scale]], dtype=np.float32
                )

                # Domain label: 0 = has NOCS
                domain_label = 0

            # Camera intrinsics
            intrinsics = np.array(f["camera_intrinsics"][0], dtype=np.float32)

            # Metadata
            if not is_negative:
                model_name = f["model_name"][()]
                if isinstance(model_name, bytes):
                    model_name = model_name.decode("utf-8")
                elif isinstance(model_name, np.bytes_):
                    model_name = model_name.decode("utf-8")
            else:
                model_name = "NEGATIVE_SAMPLE"

            sample = {
                "image": rgb,
                "masks": masks,
                "coords": coords,
                "class_ids": class_ids,
                "scales": scales,
                "domain_label": domain_label,
                "intrinsics": intrinsics,
                "model_name": model_name,
                "file_path": str(hdf5_path),
            }

            # Optionally load pose ground truth
            if self.load_poses and not is_negative:
                rotation = np.array(f["object_to_camera_rotation"], dtype=np.float32)
                translation = np.array(
                    f["object_to_camera_translation"], dtype=np.float32
                )
                sample["rotation"] = rotation
                sample["translation"] = translation

        # Apply transforms if any
        if self.transform is not None:
            sample = self.transform(sample)

        return sample


def collate_fn(batch: List[Dict]) -> Dict:
    """
    Custom collate function for batching.

    Note: Since Detectron2 handles variable-sized inputs,
    we keep most data as lists rather than stacking.
    """
    return {
        "images": [item["image"] for item in batch],
        "masks": [item["masks"] for item in batch],
        "coords": [item["coords"] for item in batch],
        "class_ids": [item["class_ids"] for item in batch],
        "scales": [item["scales"] for item in batch],
        "domain_labels": [item["domain_label"] for item in batch],
        "intrinsics": torch.stack(
            [torch.from_numpy(item["intrinsics"]) for item in batch]
        ),
        "model_names": [item["model_name"] for item in batch],
        "file_paths": [item["file_path"] for item in batch],
        # Optional pose data (if loaded)
        "rotations": [item["rotation"] for item in batch if "rotation" in item],
        "translations": [
            item["translation"] for item in batch if "translation" in item
        ],
    }


if __name__ == "__main__":
    """Test the dataset."""
    import matplotlib.pyplot as plt
    from torch.utils.data import DataLoader

    # Test loading
    dataset = NOCSDataset(
        data_dir="/home/blender/workspace/output",
        split="train",
        include_negatives=False,
        load_poses=True,  # Test pose loading
    )

    print(f"Dataset size: {len(dataset)}")

    # Test single sample
    sample = dataset[0]

    print("\n" + "=" * 70)
    print("Single Sample Test")
    print("=" * 70)
    print(f"Image shape: {sample['image'].shape}")
    print(f"Masks shape: {sample['masks'].shape}")
    print(f"Coords shape: {sample['coords'].shape}")
    print(f"Class IDs: {sample['class_ids']}")
    print(f"Scales: {sample['scales']}")
    print(f"Domain label: {sample['domain_label']}")
    print(f"Intrinsics:\n{sample['intrinsics']}")
    print(f"Model: {sample['model_name']}")

    if "rotation" in sample:
        print(f"\nRotation:\n{sample['rotation']}")
        print(f"Translation: {sample['translation']}")

    # Visualize
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # RGB
    axes[0].imshow(sample["image"])
    axes[0].set_title("RGB Image")
    axes[0].axis("off")

    # Mask
    axes[1].imshow(sample["masks"][:, :, 0], cmap="gray")
    axes[1].set_title("Instance Mask")
    axes[1].axis("off")

    # NOCS (Z-flipped)
    axes[2].imshow(sample["coords"][:, :, 0, :])
    axes[2].set_title("NOCS Map (Z-flipped)")
    axes[2].axis("off")

    plt.tight_layout()
    plt.savefig("dataset_test.png")
    print("\n✓ Saved single sample visualization")

    # Test DataLoader
    print("\n" + "=" * 70)
    print("DataLoader Test")
    print("=" * 70)

    dataloader = DataLoader(
        dataset,
        batch_size=2,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0,
    )

    for batch in dataloader:
        print(f"Batch size: {len(batch['images'])}")
        print(f"Image shapes: {[img.shape for img in batch['images']]}")
        print(f"Masks shapes: {[m.shape for m in batch['masks']]}")
        print(f"Intrinsics shape: {batch['intrinsics'].shape}")
        print(f"Model names: {batch['model_names']}")
        break

    print("\n✓ Dataset and DataLoader working correctly!")
