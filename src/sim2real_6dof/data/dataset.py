"""
PyTorch Dataset for NOCS-based 6DoF pose estimation.
Reads HDF5 files generated by the synthetic data pipeline.
"""

import io
import json
import re
import tempfile
from pathlib import Path
from typing import Dict, List, Optional

import h5py
import numpy as np
import torch
import webdataset as wds
from huggingface_hub import HfFileSystem, hf_hub_url
from torch.utils.data import Dataset


class NOCSDataset(Dataset):
    """
    Dataset for NOCS prediction from synthetic HDF5 files.

    Follows the format of the original NOCS paper's dataset.
    Each sample contains one object instance (single-instance per image).
    """

    def __init__(
        self,
        data_dir: str,
        include_negatives: bool = False,
        load_poses: bool = False,
        transform: Optional[callable] = None,
        flip_z_axis: bool = True,  # Match original NOCS convention
    ):
        """
        Args:
            data_dir: Directory containing HDF5 files (e.g., 'output/')
            include_negatives: Whether to include negative samples (background only)
            load_poses: Whether to load ground truth poses (R, t) for validation
            transform: Optional transforms to apply
        """
        self.data_dir = Path(data_dir)
        self.include_negatives = include_negatives
        self.load_poses = load_poses
        self.transform = transform
        self.flip_z_axis = flip_z_axis

        # Find all HDF5 files
        self.hdf5_files = sorted(self.data_dir.glob("*.hdf5"))

        if not self.hdf5_files:
            raise RuntimeError(f"No HDF5 files found in {self.data_dir}")

        # Build index
        self.samples = self._build_index()

        print(
            f"Loaded {len(self.samples)} samples from {len(self.hdf5_files)} HDF5 files"
        )

    def _build_index(self) -> List[Path]:
        """Build index of all valid samples."""
        samples = []

        for hdf5_path in self.hdf5_files:
            with h5py.File(hdf5_path, "r") as f:
                is_negative = f.get("is_negative", [np.array([0])])[()]
                if isinstance(is_negative, np.ndarray):
                    is_negative = is_negative[0] if is_negative.size > 0 else 0

                if is_negative and not self.include_negatives:
                    continue

                samples.append(hdf5_path)

        return samples

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict:
        hdf5_path = self.samples[idx]

        with h5py.File(hdf5_path, "r") as f:
            sample = extract_from_hdf5(f)

        return sample


def create_webdataset(
    repo_id: str,
    subfolder: str | None = "data",
    shard_ids: list[int] | None = None,
    shuffle=True,
    seed=0,
    hf_token=None,
    cache_dir=None,
):
    """
    Create a webdataset pipeline with HDF5 parsing.
    """
    fs = HfFileSystem()
    shards = fs.ls(
        f"datasets/{repo_id}/{subfolder if subfolder is not None else ''}",
        detail=False,
    )
    use_shards = []
    if shard_ids is not None:
        for shard in shards:
            match = re.match(r"output_(\d{2})\.tar\.gz", Path(shard).name)
            if match is not None:
                shard_id = int(match.group(1))
                if shard_id in shard_ids:
                    use_shards.append(shard)
    else:
        use_shards = shards
    shard_urls = [
        hf_hub_url(repo_id, Path(shard).name, subfolder=subfolder, repo_type="dataset")
        for shard in use_shards
    ]
    if hf_token is not None:
        shard_urls = [
            f"pipe:curl --connect-timeout 30 --retry 30 --retry-delay 5 -f --speed-limit 1 --speed-time 10 --retry-connrefused -s -L {shard_url} -H 'Authorization:Bearer {hf_token}'"
            for shard_url in shard_urls
        ]
    dataset = wds.WebDataset(
        shard_urls, shardshuffle=shuffle, seed=seed, cache_dir=cache_dir
    )
    if shuffle:
        dataset = dataset.shuffle(1000, rng=torch.Generator().manual_seed(seed))
    dataset = dataset.map(lambda x: parse_hdf5_sample(x))

    return dataset


def parse_hdf5_sample(sample):
    """
    Parse HDF5 from webdataset sample.

    Args:
        sample: Dict from webdataset like {'__key__': 'sample_000', 'hdf5': b'...'}

    Returns:
        Dict with parsed data
    """
    hdf5_bytes = sample["hdf5"]
    hdf5_path = f"{sample['__url__']}_{sample['__key__']}.hdf5"

    try:
        with h5py.File(io.BytesIO(hdf5_bytes), "r") as f:
            parsed_data = extract_from_hdf5(f, hdf5_path=hdf5_path)
    except (OSError, Exception):
        with tempfile.NamedTemporaryFile(suffix=".hdf5", delete=True) as tmp:
            tmp.write(hdf5_bytes)
            tmp.flush()
            with h5py.File(tmp.name, "r") as f:
                parsed_data = extract_from_hdf5(f, hdf5_path=hdf5_path)

    return parsed_data


def extract_from_hdf5(
    f, flip_z_axis=True, transform=None, load_poses=False, hdf5_path=None
):
    """
    Load a single sample.

    Returns dict with format matching original NOCS dataset:
        - image: RGB (H, W, 3), uint8
        - masks: Instance masks (H, W, 1), bool - note: single instance
        - coords: NOCS maps (H, W, 1, 3), float32 [0, 1]
        - class_ids: Class labels (1,), int64 - note: always 0 (mug)
        - scales: Object scale (1, 3), float32
        - domain_label: 0 (has NOCS) or 1 (no NOCS)
        - intrinsics: Camera K (3, 3), float32

    Optional (if load_poses=True):
        - rotation: (3, 3), float32
        - translation: (3,), float32
    """
    # RGB image (H, W, 3 or 4) -> (H, W, 3)
    rgb = np.array(f["colors"])
    if rgb.shape[-1] == 4:
        rgb = rgb[:, :, :3]

    # Check if negative sample
    is_negative = f.get("is_negative", [np.array([0])])[()]
    if isinstance(is_negative, np.ndarray):
        is_negative = is_negative[0] if is_negative.size > 0 else 0

    if is_negative:
        # Return empty masks/coords for negative samples
        h, w = rgb.shape[:2]
        masks = np.zeros((h, w, 0), dtype=bool)
        coords = np.zeros((h, w, 0, 3), dtype=np.float32)
        class_ids = np.array([], dtype=np.int64)
        scales = np.zeros((0, 3), dtype=np.float32)
        domain_label = 1  # No NOCS for negatives
    else:
        # Instance mask (H, W) -> (H, W, 1)
        mask = np.array(f["instance_mask"])
        masks = (mask > 0)[:, :, np.newaxis]

        # NOCS map (H, W, 3 or 4) -> (H, W, 3)
        nocs = np.array(f["nocs"])[:, :, :3]

        if flip_z_axis:
            # CRITICAL: Flip Z-axis to match official NOCS format
            nocs[:, :, 2] = 1 - nocs[:, :, 2]

        # Mask out background and add instance dimension
        # NOCS map (H, W, 3) -> (H, W, 1, 3)
        coords = (nocs * masks).astype(np.float32)[:, :, np.newaxis, :]

        # Class ID (always 0 for mug in single-class case)
        class_ids = np.array([0], dtype=np.int64)

        # Scale from metadata
        metadata_raw = f["metadata"][()]
        if isinstance(metadata_raw, bytes):
            metadata_str = metadata_raw.decode("utf-8")
        elif isinstance(metadata_raw, np.bytes_):
            metadata_str = metadata_raw.decode("utf-8")
        else:
            metadata_str = str(metadata_raw)
        metadata = json.loads(metadata_str)
        object_scale = metadata.get("object_scale", 1.0)

        # Scales shape: (1, 3) - uniform scale in all dimensions
        scales = np.array(
            [[object_scale, object_scale, object_scale]], dtype=np.float32
        )

        # Domain label: 0 = has NOCS
        domain_label = 0

    # Camera intrinsics
    intrinsics = np.array(f["camera_intrinsics"][0], dtype=np.float32)

    # Metadata
    if not is_negative:
        model_name = f["model_name"][()]
        if isinstance(model_name, bytes):
            model_name = model_name.decode("utf-8")
        elif isinstance(model_name, np.bytes_):
            model_name = model_name.decode("utf-8")
    else:
        model_name = "NEGATIVE_SAMPLE"

    sample = {
        "image": rgb,
        "masks": masks,
        "coords": coords,
        "class_ids": class_ids,
        "scales": scales,
        "domain_label": domain_label,
        "intrinsics": intrinsics,
        "model_name": model_name,
        "file_path": str(hdf5_path) if hdf5_path else "",
    }

    # Optionally load pose ground truth
    if load_poses and not is_negative:
        rotation = np.array(f["object_to_camera_rotation"], dtype=np.float32)
        translation = np.array(f["object_to_camera_translation"], dtype=np.float32)
        sample["rotation"] = rotation
        sample["translation"] = translation

    # Apply transforms if any
    if transform is not None:
        sample = transform(sample)

    return sample
